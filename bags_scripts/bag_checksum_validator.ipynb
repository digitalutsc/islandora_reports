{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "## Script to manually generate checksum for bags\n",
    "## and check if internal checksum matches\n",
    "##################################################\n",
    "## GNU General Public License, version 2\n",
    "## Author: UTSC DSU \n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import os\n",
    "import zipfile\n",
    "import hashlib\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_file_version(zip, datastream_ids_to_check):\n",
    "    #list of files matching a given id\n",
    "    id_matching_files = []\n",
    "    \n",
    "    #latest version of the file found and the version #\n",
    "    latest_file = ''\n",
    "    latest_ver = 0\n",
    "    \n",
    "    #list of all the latest files given\n",
    "    latest_files = []\n",
    "    \n",
    "    #find latest version of all the datastream_ids_to_check\n",
    "    all_files = zip.namelist()\n",
    "    for id in datastream_ids_to_check:\n",
    "        #find all files matching the id\n",
    "        for f in all_files:\n",
    "            file_parts = f.split('.')\n",
    "            if (file_parts[0] == id):\n",
    "                id_matching_files.append(f)\n",
    "        #loop through all the files and find the latest version\n",
    "        #print(\"[%s]\" % ', '.join(map(str, id_matching_files)))\n",
    "        for f in id_matching_files:\n",
    "            file_parts = f.split('.')\n",
    "            latest_file = f\n",
    "            latest_ver = file_parts[1]\n",
    "            if(file_parts[1] > latest_ver):\n",
    "                latest_file = f\n",
    "                latest_ver = file_parts[1]\n",
    "        \n",
    "        #if file exists add it otherwise add the id\n",
    "        if (latest_file == ''):\n",
    "            latest_files.append(id)\n",
    "        else:\n",
    "            latest_files.append(latest_file)\n",
    "            \n",
    "        #set everything back for next set of ids\n",
    "        latest_file = ''\n",
    "        latest_ver = 0\n",
    "        id_matching_files = []\n",
    "        \n",
    "    return latest_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually generate checksums for datastream_ids_to_check files \n",
    "#within the initial_zip data folder \n",
    "#(gets latest version of file or just the file you input)\n",
    "def manually_generate_checksums(initial_zip, datastream_ids_to_check, bag_zip_location):\n",
    "    #open initial zip file and save it as init_zip\n",
    "    init_zip = zipfile.ZipFile(initial_zip)\n",
    "\n",
    "    #path for bag data content within init_zip\n",
    "    #bag_full_path = os.path.splitext(initial_zip)[0]+'/data/'\n",
    "    #bag_location = bag_full_path.split('/')[-3] +'/data/'\n",
    "   # print(bag_location)\n",
    "    #bag_name = os.path.splitext(initial_zip.split('/')[-1][4:])[0]\n",
    "\n",
    "    #the location for the foxml files\n",
    "    #bag_zip_location = bag_location + bag_name + '_foxml_atomzip.zip'\n",
    "    \n",
    "    #open zip file within data\n",
    "    zip = zipfile.ZipFile(init_zip.open(bag_zip_location))\n",
    "    print(\"\\nGenerating checksums for bag stored at \" + bag_zip_location)\n",
    "    \n",
    "    latest_files = find_latest_file_version(zip, datastream_ids_to_check)\n",
    "    \n",
    "    #list of files to check through can set to zip.namelist() \n",
    "    #to look through all files within the foxml folder\n",
    "    filenames = latest_files\n",
    "    #filenames = zip.namelist()\n",
    "\n",
    "    #store the files and their checksums in a dict\n",
    "    manual_checksum_list = {}\n",
    "    #loop for checking all files in the filenames list\n",
    "    for filename in filenames:\n",
    "        filename = filename.strip()\n",
    "        #try to generate checksum for file (if it exists)\n",
    "        try:\n",
    "            file = zip.open(filename)\n",
    "\n",
    "            #generate md5 checksum for the file\n",
    "            hash_md5 = hashlib.md5()\n",
    "            for chunk in iter(lambda: file.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "            generated_checksum = hash_md5.hexdigest()\n",
    "            #print(generated_checksum)\n",
    "            \n",
    "            #print('Generated manual checksum for \\'' + filename + '\\'')\n",
    "\n",
    "            manual_checksum_list[os.path.basename(filename)] = generated_checksum\n",
    "        except:\n",
    "            #if file doesnt exist print to output\n",
    "            print('File named \\'' + filename + '\\' does not exist (ignoring it)')\n",
    "\n",
    "    #print('Finished manually generating checksum for all files within data folder')\n",
    "    return manual_checksum_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get internal checksum stored in the foxml.xml file and compare them\n",
    "#against manually generated checksum  (calls the manually_generate_checksum method)\n",
    "def datastream_checksum_validator(initial_zip_loc, datastream_ids_to_check):\n",
    "    \n",
    "    status = True\n",
    "    \n",
    "    #open initial zip file\n",
    "    init_zip = zipfile.ZipFile(initial_zip_loc)\n",
    "\n",
    "    #path for bag contents within init_zip\n",
    "    bag_full_path = os.path.splitext(initial_zip_loc)[0]+'/data/'\n",
    "    bag_location = bag_full_path.split('/')[-3] +'/data/'\n",
    "\n",
    "    bag_name = os.path.splitext(initial_zip_loc.split('/')[-1][4:])[0]\n",
    "    \n",
    "    #get root of xml tree\n",
    "    #bag_xmldata_path = bag_location+'foxml.xml'\n",
    "    \n",
    "    bag_zip_location = bag_location + bag_name + '_foxml_atomzip.zip'\n",
    "    \n",
    "    #get list of foxmls within the data folder\n",
    "    foxml_file_list = []\n",
    "    for file in init_zip.namelist():\n",
    "        if file.startswith(bag_location) and file.endswith('_foxml_atomzip.zip'):\n",
    "            foxml_file_list.append(file)\n",
    "      \n",
    "    #loop through every foxml file\n",
    "    for bag_zip_location in foxml_file_list:\n",
    "        zip = zipfile.ZipFile(init_zip.open(bag_zip_location))\n",
    "\n",
    "        bag_xmldata_path = 'atommanifest.xml'\n",
    "\n",
    "        #call to generate checksums from files in data_stream_ids_to_check \n",
    "        manual_checksum_list = manually_generate_checksums(initial_zip_loc, datastream_ids_to_check, bag_zip_location)\n",
    "\n",
    "        #find the root of the xml file\n",
    "        #root = ET.parse(zip.open(bag_xmldata_path)).getroot()\n",
    "        root = ET.parse(zip.open('atommanifest.xml')).getroot()\n",
    "\n",
    "        #find the \"datastreamVersion\" tag in the xml file (contains filenames)\n",
    "        datastreamVersion_list = root.findall(\"./{http://www.w3.org/2005/Atom}entry\")\n",
    "\n",
    "     #   print(len(datastreamVersion_list))\n",
    "     #   print(\"testing HERE: \" +datastreamVersion_list)\n",
    "\n",
    "        #print('Comparing with internal checksum stored at: ' + bag_xmldata_path)\n",
    "\n",
    "        #loop through the files that have their manually generated checksum\n",
    "        for filename in manual_checksum_list:\n",
    "            #loop through find all datastreamVersion tags (contains filename)\n",
    "            #can redo to make it run faster... \n",
    "            for c_tag in datastreamVersion_list:\n",
    "                generated_checksum = manual_checksum_list[filename]\n",
    "\n",
    "                #find contentDigest tags (contains internally stored checksum)\n",
    "                digest_tag_all =c_tag.findall ('./{http://www.w3.org/2005/Atom}category')\n",
    "                file_name_tag = c_tag.findall('./{http://www.w3.org/2005/Atom}content')\n",
    "                if len(file_name_tag) != 0:\n",
    "                    #print(file_name_tag)\n",
    "                    tag_file_name = file_name_tag[0].get('src')\n",
    "                    tag_file_name = os.path.splitext(tag_file_name)[0]\n",
    "                else:\n",
    "                    tag_file_name = ''\n",
    "                tag_internal_checksum = ''\n",
    "                for digest_tag in digest_tag_all:\n",
    "                    if digest_tag.get('scheme') == \"info:fedora/fedora-system:def/model#digest\":\n",
    "                        tag_internal_checksum = digest_tag.get('term')\n",
    "\n",
    "                #check if internally stored checksum exists\n",
    "                #if so find the two checksums match given they have the same filename\n",
    "                #print(\"TESTING \" + tag_file_name +\" \"+ tag_internal_checksum +\" \"+ os.path.splitext(filename)[0])\n",
    "                if (tag_internal_checksum != '' and\\\n",
    "                    tag_file_name == os.path.splitext(filename)[0] and\\\n",
    "                    tag_internal_checksum == generated_checksum): \n",
    "\n",
    "                    print(\"PASS: File \" + tag_file_name +  \" with internal checksum \\'\"\\\n",
    "                          + tag_internal_checksum + \"\\' matches manual checksum\")\n",
    "\n",
    "                #if the checksums dont match print the non matching one and return false\n",
    "                elif (tag_internal_checksum != '' and\\\n",
    "                    tag_file_name == os.path.splitext(filename)[0] and\\\n",
    "                    tag_internal_checksum != generated_checksum):\n",
    "\n",
    "                    print(\"FAIL: File \" + tag_file_name + \\\n",
    "                          \" has a conflicting generated and internal checksum as follows:\")\n",
    "                    data = {'internally stored checksum': [tag_internal_checksum], \\\n",
    "                            'manually generated checksum': [generated_checksum]}\n",
    "                    df = pd.DataFrame(data)\n",
    "                    print(df)\n",
    "                    status = False\n",
    "            #print('\\n')\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
